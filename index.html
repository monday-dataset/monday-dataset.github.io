<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-80J6X8CEKR"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-80J6X8CEKR');
  </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>MONDAY: Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css">
    <link rel="icon" href="images/monday_logo.ico" type="image/x-icon">

    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/latex.min.js"></script>
    <script>hljs.highlightAll();</script>

    <style>
        body {
            font-family: 'Arial', sans-serif;
            padding-top: 80px;
            font-size: 1.1em;
        }

        .custom-navbar {
            background-color: #f8f9fa;
        }

        .title {
            font-family: 'Google Sans', sans-serif;
            font-weight: bold;
            font-size: 2em;
            padding-bottom: 10px;
        }

        .section-title {
            font-family: 'Google Sans', sans-serif;
            padding-top: 20px;
            padding-bottom: 20px;
            scroll-margin-top: 60px;
        }

        .center-row {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            align-items: center;
        }

        .author {
            text-align: center;
            display: inline-block;
            font-size: 1.1em;
          }

        .author img {
            width: 100px;
            height: 100px;
            border-radius: 50%;
            object-fit: cover;
        }

        .author-name {
            margin-top: 8px;
            font-weight: bold;
        }

        .affiliation {
            font-size: 1em;
            color: #666;
        }

        .conference {
            text-align: center;
            display: inline-block;
            font-size: 1.1em;
        }

        .links {
            margin-top: 20px;
            margin-bottom: 40px;
            text-align: center;
        }

        .teaser-row {
            margin-bottom: 30px;
        }

        .teaser-img {
            width: 45%;
            border-radius: 5px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }

        .img-wrapper {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 20px; /* space between gifs */
            margin-bottom: 10px;
        }

        .dataset-example {
            margin-bottom: 30px;
            text-align: center;
        }

        .dataset-example img {
            width: 100%;
            border-radius: 5px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
            margin-bottom: 10px;
        }

        .pipeline-img {
            width: 100%;
            margin: 20px 0;
            border-radius: 5px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }

        .results-img {
            width: 80%;
            margin-bottom: 15px;
            border-radius: 5px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }

        .caption {
            font-size: 0.9em;
            color: #666;
            text-align: center;
            margin-bottom: 20px;
        }

        footer {
            margin-top: 50px;
            margin-bottom: 30px;
            text-align: center;
            color: #666;
        }

        .stats-box {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 20px;
            text-align: center;
            font-size: 1em;
        }

        .stats-number {
            font-size: 2.5em;
            font-weight: bold;
            color: #4c4b45;
        }

        .stats-label {
            font-size: 1.1em;
            color: #666;
        }

        .table-wrapper {
          display: flex;
          justify-content: center; /* Horizontal centering */
          align-items: center;     /* Vertical centering */
        }

        table.results-table {
            width: 100%;
            margin-bottom: 20px;
        }

        .results-table th {
            background-color: #f8f9fa;
            text-align: center;
        }

        .results-table td {
            text-align: center;
        }

        .highlight {
            background-color: #fffacd;
            padding: 2px;
        }

        pre {
            background-color: #f5f5f5;
            border-radius: 5px;
            padding: 15px;
            margin: 15px 0;
            overflow-x: auto;
        }

        code {
            font-family: 'Courier New', Courier, monospace;
            color: #333;
        }

        p code, li code {
            background-color: #f5f5f5;
            padding: 2px 4px;
            border-radius: 3px;
        }

        .btn-big {
            margin: 5px;
            padding: 8px 16px;
            font-weight: normal;
            font-size: 1.1em;
            color: #333;
        }

        .btn-paper:hover {
          background-color: #B31B1B;
          color: white;
          border-color: #B31B1B;
        }

        .btn-code:hover {
          background-color: #343a40;
          color: white;
          border-color: #343a40;
        }

        .btn-data:hover {
          background-color: #1f5fa0;
          color: white;
          border-color: #1f5fa0;
        }



    </style>
</head>
<body>
      <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light fixed-top"> -->
        <nav class="navbar navbar-expand-lg navbar-light custom-navbar fixed-top">
        <div class="container">
            <a class="navbar-brand" href="#"><img src="images/monday_logo.png" width="25"/> MONDAY</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="#overview">Overview</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#pipeline">Pipeline</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#dataset">Dataset</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#experiments">Experiments</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#download">Download</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <div class="container">
        <div class="row">
            <div class="col-md-12 text-center">
                <h1 class="title">
                <img src="images/monday_logo.png" width="40"/>
                MONDAY: Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents
                </h1>
            </div>
        </div>

       <div class="row">
            <div class="col-md-12">
              <div class="center-row">
                <span class="author">
                  <a href="https://yunseokjang.github.io/">
                    <span class="fancy_text_color">Yunseok Jang</span>
                  </a><sup>*1</sup>
                </span>&nbsp;&nbsp;

                <span class="author">
                  <a href="https://yedasong.com/">
                    <span class="fancy_text_color">Yeda Song</span>
                  </a><sup>*1</sup>
                </span>&nbsp;&nbsp;

                <span class="author">
                  <a href="https://sites.google.com/view/sungryull">
                    <span class="fancy_text_color">Sungryull Sohn</span>
                  </a><sup>2</sup>
                </span>&nbsp;&nbsp;

                <span class="author">
                  <a href="https://lajanugen.github.io/">
                    <span class="fancy_text_color">Lajanugen Logeswaran</span>
                  </a><sup>2</sup>
                </span>&nbsp;&nbsp;
              </div>
              <div class="center-row">
                <span class="author">
                  <a href="https://tiangeluo.github.io/">
                    <span class="fancy_text_color">Tiange Luo</span>
                  </a><sup>1</sup>
                </span>&nbsp;&nbsp;

                <span class="author">
                  <a href="https://dkkim93.github.io/">
                    <span class="fancy_text_color">Dong-Ki Kim</span>
                  </a><sup>2</sup>
                </span>&nbsp;&nbsp;

                <span class="author">
                  <a href="https://sites.google.com/view/kyunghoonbae/home">
                    <span class="fancy_text_color">Kyunghoon Bae</span>
                  </a><sup>2</sup>
                </span>&nbsp;&nbsp;

                <span class="author">
                  <a href="https://web.eecs.umich.edu/~honglak/">
                    <span class="fancy_text_color">Honglak Lee</span>
                  </a><sup>1,2</sup>
                </span>
              </div>

              <div class="center-row">
                <span class="affiliation"><sup>1</sup>University of Michigan</span>&nbsp;&nbsp;
                <span class="affiliation"><sup>2</sup>LG AI Research</span>
              </div>

              <div class="center-row">
                <span class="affiliation">*Equal contribution</span>
              </div>

              <div class="center-row">
                <span class="conference">CVPR 2025</span>
              </div>


              <div class="row">
                  <div class="col-md-12">
                      <div class="links">
                          <span class="link-block">
                            <a href="https://arxiv.org/abs/2505.12632" class="btn btn-outline-dark btn-paper btn-big" role="button">
                              <img src="https://cdn.simpleicons.org/arxiv" alt="ArXiv" width="20" height="20">
                              <!-- &#128221;  -->
                              &nbsp; Paper &nbsp;
                            </a>&nbsp;&nbsp;

                            <a href="https://github.com/runamu/monday" class="btn btn-outline-dark btn-code btn-big" role="button">
                              <img src="https://cdn.simpleicons.org/github" alt="GitHub" width="20" height="20">
                              &nbsp; Code &nbsp;
                            </a>&nbsp;&nbsp;

                            <a href="https://huggingface.co/datasets/runamu/MONDAY" class="btn btn-outline-dark btn-data btn-big" role="button">
                              <img src="https://cdn.simpleicons.org/huggingface" alt="GitHub" width="20" height="20">
                              &nbsp; Data &nbsp;
                            </a>
                          </span>
                      </div>
                  </div>
              </div>

            </div>
        </div>

        <div class="row teaser-row">
          <div class="col-md-12 text-center">
              <div class="img-wrapper">
                  <img src="images/example3_figure_g.gif" alt="Teaser" class="teaser-img">
                  <img src="images/example6_figure_h.gif" alt="Teaser" class="teaser-img">
              </div>
              <p class="caption">
                  MONDAY provides a diverse Android/iOS navigation trajectories for training pixel-based GUI agents, extracted from instructional videos. It comprises 20,320 sequences and 312,754 frames, and the GIFs above showcase the automatic annotations.
              </p>
          </div>
      </div>


        <div class="row center-row">
          <div class="col-md-4">
            <div class="stats-box">
              <div class="stats-number">313K</div>
              <div class="stats-label">Annotated Frames</div>
            </div>
          </div>
          <div class="col-md-4">
            <div class="stats-box">
              <div class="stats-number">2.5K</div>
              <div class="stats-label">Apps</div>
            </div>
          </div>
          <div class="col-md-4">
            <div class="stats-box">
              <div class="stats-number">Android & iOS</div>
              <div class="stats-label">Multi-Platform</div>
            </div>
          </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <h2 class="section-title" id="overview">Overview</h2>

                <p>
                  <i>MONDAY</i> (Mobile OS Navigation Task Dataset for Agents from YouTube) is a cross-platform dataset for training vision-language navigation agents on real-world mobile interfaces. We tackle key challenges in building robust GUI agents by collecting diverse, realistic data at scale—without requiring direct access to the environments.
                </p>

                <p>
                  Our data is collected from publicly available YouTube videos using a robust, fully-automated pipeline that ensures high quality across various mobile OS platforms (Android and iOS), OS versions, and user configurations.
                </p>

                <p>
                  MONDAY is:
                </p>

                <ul>
                  <li><strong>Diverse:</strong> Covers both Android and iOS with data from 2,479 apps, including system apps and a wide range of GUI configurations.</li>
                  <li><strong>Large-Scale:</strong> Includes 20,320 sequences and 312,754 annotated frames.</li>
                  <li><strong>Real-World:</strong> Tasks curated from CommonCrawl web posts (e.g.,
                    <a href="https://huggingface.co/datasets/allenai/c4">C4</a>,
                    <a href="https://huggingface.co/datasets/allenai/dolma">Dolma</a> datasets) and videos sourced from YouTube capture authentic, real-world mobile interactions.</li>
                </ul>

                <p>
                  In contrast to traditional emulator-based data collection methods, MONDAY introduces a new paradigm in GUI agents: collecting human demonstration from <i>videos</i>, with <strong>zero access</strong> to the GUI systems.
                  Models trained on our dataset demonstrate strong generalization on both public and out-of-distribution datasets—highlighting the effectiveness and value of this approach.
                </p>
            </div>
        </div>

        <div class="row">
          <div class="col-md-12">
              <h2 class="section-title" id="pipeline">Data Collection</h2>
              <img src="images/main-ver8.png" alt="Pipeline" class="pipeline-img">
              <p class="caption">
                Core components of the MONDAY data collection framework, showing scene transition detection followed by a 3-step action identification process.
              </p>
              <p>
                Our data collection framework consists of several carefully designed stages to extract high-quality mobile OS navigation data from real-world videos.
                The process includes these key steps:
              </p>
              <ol>
                  <li><strong>Mobile Navigation Video Collection:</strong> We gather real-world instructional videos from YouTube based on user-written task queries mined from CommonCrawl web posts (e.g.,
                    <a href="https://huggingface.co/datasets/allenai/c4">C4</a>,
                    <a href="https://huggingface.co/datasets/allenai/dolma">Dolma</a> datasets). Videos are filtered to include only mobile phone content, with clean views and narration.</li>
                  <li><strong>Scene Transition Detection:</strong> We isolate phone screens and detect transitions using OCR-based text change analysis, ensuring robust segmentation of task steps across varying UI layouts.</li>
                  <li><strong>UI Element Detection:</strong> Detected screens undergo UI element extraction using
                    <a href="https://github.com/IDEA-Research/GroundingDINO">GroundingDINO</a> and
                    <a href="https://github.com/PaddlePaddle/PaddleOCR">PaddleOCR</a>, followed by heuristic filtering to identify actionable elements.</li>
                  <li><strong>3-step Action Identification:</strong> Actions are annotated using a novel three-step method: (1) summarizing scenes, (2) initial action prediction using Set-of-Marks (SoM), and (3) refined action localization with zoomed-in views.</li>
              </ol>
          </div>
      </div>

        <div class="row">
            <div class="col-md-12">
              <h2 class="section-title" id="dataset">Dataset Statistics</h2>
                <p>
                  Below are tables and figures presenting some statistics of MONDAY:
                </p>
            </div>

            <div class="center-row">
              <div class="col-md-6">
                    <img src="images/split_table.png" alt="Video Duration" class="pipeline-img">
              </div>
              <div class="col-md-4">
                <img src="images/app_table.png" alt="Action Types" class="pipeline-img">
              </div>
            </div>

            <div class="center-row">
              <div class="col-md-5">
                    <img src="images/duration_distribution.png" alt="Video Duration" class="pipeline-img">
              </div>
              <div class="col-md-5">
                <img src="images/action_pie_chart.png" alt="Action Types" class="pipeline-img">
              </div>
            </div>

            <div class="col-md-12">
              <p class="caption">
                      (Top-Left) Number of videos across different splits. MONDAY maintains approximately a 50:50 ratio of iOS to Android videos in each split. <br>
                      (Top-Right) App statistics in MONDAY videos. MONDAY includes both native OS system apps and third-party apps. <br>
                      (Bottom-Left) Distribution of video duration in minutes. Red vertical dotted line stands for the average duration of 2.66 minutes. <br>
                      (Bottom-Right) Action type distribution. Touch actions dominate at 79.83%, followed by scroll (8.53%) and other actions.
                  </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <h2 class="section-title" id="experiments">Experiments and Results</h2>
                We evaluate both our dataset collection method and models trained on MONDAY through comprehensive experiments.
                To evaluate our data collection method, we manually annotated 100 videos.
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-12 text-center">
              <h3>Dataset Collection Method Evaluation</h3>
            </div>
          <div class="col-md-4">
                <img src="images/scene_transition_detection.png" alt="Scene Transition Detection" class="pipeline-img">
            </div>
          <div class="col-md-4">
                <img src="images/ui_element_detection.png" alt="UI Element Detection" class="pipeline-img">
            </div>
          <div class="col-md-4">
                <img src="images/action_identification.png" alt="Action Identification" class="pipeline-img">
            </div>
          <div class="col-md-12">
            <p class="caption">
                  (Left) Our OCR-based approach significantly outperforms baselines by leveraging text content changes rather than traditional visual features.
                  <br>
                  (Middle) Our UI element detection is robust, accurately identifying home screen icons and bottom-positioned UI elements that OmniParser frequently misses.
                  <br>
                  (Right) Our multi-image 3-step approach outperforms simplified variants.
              </p>
          </div>
        </div>

        <div class="row">
          <div class="col-md-12 text-center">
              <h3>Mobile Navigation Agent Evaluation</h3>
          </div>

          <div class="center-row">
            <div class="col-md-10">
                  <img src="images/model_evaluation.png" alt="Mobile Navigation Agent Evaluation" class="pipeline-img">
            </div>
          </div>
          <div class="col-md-12">
            <p class="caption">
            Step accuracies of the original pre-trained models (SeeClick, Llama-3.2) vs. the corresponding MONDAY-induced variants (SeeClick-MONDAY, Llama-3.2-MONDAY).
            Models finetuned from MONDAY-induced variants mostly outperform the baselines and generalize well to an unseen mobile platform (Windows Mobile).
            </p>
        </div>

      </div>

        <div class="row">
            <div class="col-md-12">
                <h2 class="section-title" id="download">Download and Usage</h2>
                <p>
                  You can download our <a href="https://huggingface.co/datasets/runamu/MONDAY" target="_blank">dataset</a> from Hugging Face:
                </p>
                <p>
<pre><code class="language-python">from datasets import load_dataset
dataset_dict = load_dataset("runamu/MONDAY")
</code></pre>
                </p>
                <p>
                  You'll have to <strong>download the videos by yourself</strong> to use our dataset for agent training and evaluation.
                  To learn how to use the dataset, check out our
                  <a href="https://github.com/runamu/monday" target="_blank">code repository</a> on GitHub.
                </p>

                <p>
                  For detailed information about the dataset fields, visit the
                  <a href="https://huggingface.co/datasets/runamu/MONDAY#dataset-card-for-monday" target="_blank">dataset card</a>
                  on Hugging Face</a>.
                </p>

            </div>
        </div>

        <div class="row">
          <div class="col-md-12">
              <h2 class="section-title" id="download">Citation</h2>
              <p>
<pre><code class="language-latex">@inproceedings{jang2025_monday,
  title={{Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents}},
  author={Jang, Yunseok and Song, Yeda and Sohn, Sungryull and Logeswaran, Lajanugen and Luo, Tiange and Kim, Dong-Ki and Bae, Kyunghoon and Lee, Honglak},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025}
}</code></pre>
              </p>
          </div>
      </div>

      <div style="margin-top: 30px;"></div>

        <div align="center" class="container">
          <div class="columns is-centered">
            <span class="author" style="font-size: 200%; margin-bottom: 10px;">
            </span>
          </div>
          <div class="columns is-centered">
            <img src="images/monday_logo.png", width="200"/>
          </div>
        </div>

        <footer>
          <div align="center" class="container">
            <div class="columns is-centered">
                <div class="content is-small">
                    This website template is partly based on <a href="https://xwang.dev/mint-bench/">MINT</a>.
                  <p>&copy; 2025 The MONDAY Authors</p>
                </div>
            </div>
        </div>

        </footer>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>